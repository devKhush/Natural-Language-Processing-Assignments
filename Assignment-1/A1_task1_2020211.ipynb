{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_dir = os.path.join(os.getcwd(), 'data')\n",
    "input_file = os.path.join(input_dir, 'corpus.txt')\n",
    "\n",
    "with open(input_file, 'r') as f:\n",
    "    corpus = f.readlines()\n",
    "    for i in range(len(corpus)):\n",
    "        corpus[i] = corpus[i][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i stand here i feel empty a class post count link href http mooshilu\n",
      "i literally just text tychelle to see if she wants to hang out because reading what i just wrote about my nonexistent social life made me feel so pathetic\n"
     ]
    }
   ],
   "source": [
    "print(corpus[0])\n",
    "print(corpus[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of lines in corpus: 2400\n"
     ]
    }
   ],
   "source": [
    "print('Total number of lines in corpus:', len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need to clean corpus: False\n"
     ]
    }
   ],
   "source": [
    "need = False\n",
    "for line in corpus:\n",
    "    if any(c.isupper() for c in line) or any(c.isdigit() for c in line) or any(c in ['!', '?', '.', ',', ':'] for c in line):\n",
    "        need = True\n",
    "print('Need to clean corpus:', need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.all_merges = dict()\n",
    "        self.vocabulary = []\n",
    "        self.word_frequencies = dict()\n",
    "        self.word_splits = dict()\n",
    "    \n",
    "    def learn_vocablury(self, corpus:List[str], num_merges:int):\n",
    "        '''\n",
    "        Learn the vocabulary from the corpus using the BPE algorithm\n",
    "        '''\n",
    "        word_frequencies = self.__count_words(corpus)\n",
    "        vocabulary = self.__create_base_vocabulary(word_frequencies)\n",
    "        word_splits = self.__split_words(word_frequencies)\n",
    "        all_merges = dict()\n",
    "        # print('Initial vocabulary:', vocabulary)\n",
    "        # print('Length of initial vocabulary:', len(vocabulary))\n",
    "        # print('Initial word splits:', word_splits)\n",
    "        # print()\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            # print('Iteration:', i+1)\n",
    "            pair_frequencies = self.__compute_pair_frequencies(word_frequencies, word_splits)\n",
    "            max_freq_pair = max(pair_frequencies, key=pair_frequencies.get) if pair_frequencies else None\n",
    "            # print('Pair frequencies:', pair_frequencies)\n",
    "            # print('Max frequency pair:', max_freq_pair)\n",
    "            if max_freq_pair is None:\n",
    "                return\n",
    "            \n",
    "            max_freq = pair_frequencies[max_freq_pair]\n",
    "            word_splits = self.__merge_pair(max_freq_pair, word_splits, word_frequencies)\n",
    "            vocabulary.append(''.join(max_freq_pair))\n",
    "            all_merges[max_freq_pair] = {'merge': ''.join(max_freq_pair), 'frequency': max_freq}\n",
    "            \n",
    "            # print('Merged pair frequency:', max_freq)\n",
    "            # print('New vocabulary:', vocabulary)\n",
    "            # print('Length of new vocabulary:', len(vocabulary))\n",
    "            # print('New word splits:', word_splits)\n",
    "            # print()\n",
    "            \n",
    "        self.vocabulary = vocabulary\n",
    "        self.word_frequencies = word_frequencies\n",
    "        self.all_merges = all_merges\n",
    "        self.word_splits = word_splits\n",
    "    \n",
    "    def tokenize(self, sentence:str):\n",
    "        '''\n",
    "        Tokenize a sentence using the vocabulary learned from the corpus\n",
    "        '''\n",
    "        words = sentence.split()\n",
    "        test_word_splits = []\n",
    "        for word in words:\n",
    "            word = word + '$'\n",
    "            test_word_splits.append([character for character in word])\n",
    "        \n",
    "        for merge_pair, merge_info in self.all_merges.items():\n",
    "            for j in range(len(test_word_splits)):\n",
    "                split = test_word_splits[j]\n",
    "                i = 0\n",
    "                while i < len(split) - 1:\n",
    "                    if split[i] == merge_pair[0] and split[i+1] == merge_pair[1]:\n",
    "                        split = split[:i] + [merge_info['merge']] + split[i+2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                test_word_splits[j] = split\n",
    "\n",
    "        tokenize_sentence = []\n",
    "        [tokenize_sentence.extend(split) for split in test_word_splits]\n",
    "        return tokenize_sentence                \n",
    "        \n",
    "    \n",
    "    def __count_words(self, corpus:List[str]):\n",
    "        '''\n",
    "        Count the frequency of each word in the corpus\n",
    "        '''\n",
    "        word_frequencies = dict()\n",
    "        for sentence in corpus:\n",
    "            words_list = sentence.split()\n",
    "            for word in words_list:\n",
    "                word = word + '$'\n",
    "                word_frequencies[word] = word_frequencies.get(word, 0) + 1\n",
    "        return word_frequencies\n",
    "                \n",
    "    def __create_base_vocabulary(self, word_frequencies:Dict[str, int]):\n",
    "        '''\n",
    "        Create a base vocabulary from the words in the corpus which contains all the characters in the corpus\n",
    "        '''\n",
    "        vocabulary = set()\n",
    "        for word in word_frequencies:\n",
    "            for character in word:\n",
    "                vocabulary.add(character)\n",
    "        vocabulary = list(vocabulary)\n",
    "        return vocabulary\n",
    "    \n",
    "    def __split_words(self, word_frequencies:Dict[str, int]):\n",
    "        '''\n",
    "        Split each word in the corpus into a list of characters\n",
    "        '''\n",
    "        word_splits = dict()\n",
    "        for word in word_frequencies:\n",
    "            word_splits[word] = [character for character in word]\n",
    "        return word_splits\n",
    "    \n",
    "    def __compute_pair_frequencies(self, word_frequencies:Dict[str, int], word_splits:Dict[str, List[str]]):\n",
    "        '''\n",
    "        Compute the frequency of each pair of characters in the corpus\n",
    "        '''\n",
    "        pair_frequencies = dict()\n",
    "        for word in word_frequencies:\n",
    "            split = word_splits[word]\n",
    "            for i in range(len(split) - 1):\n",
    "                pair = (split[i], split[i+1])\n",
    "                pair_frequencies[pair] = pair_frequencies.get(pair, 0) + word_frequencies[word]\n",
    "        return pair_frequencies\n",
    "    \n",
    "    def __merge_pair(self, pair:Tuple[str, str], word_splits:Dict[str, List[str]], word_frequencies:Dict[str, int]):\n",
    "        '''\n",
    "        Given the most frequent pair of token, merge them into a single token in all the words in the corpus\n",
    "        '''\n",
    "        new_word = ''.join(pair)\n",
    "        for word in word_frequencies:\n",
    "            split = word_splits[word]\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i+1] == pair[1]:\n",
    "                    split = split[:i] + [new_word] + split[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "            word_splits[word] = split\n",
    "        return word_splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intialize the Tokenizer with corpus and number of merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "byte_pair_tokenizer = Tokenizer()\n",
    "byte_pair_tokenizer.learn_vocablury(corpus, 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['that$', 'is$', 'my$', 'life$', 'any', 'time$', 'after$', '5', 'p', 'm$', 'right$', 'so$', 'that$', 'i$', 'need$', 'to$']\n"
     ]
    }
   ],
   "source": [
    "ans = byte_pair_tokenizer.tokenize('that is my life anytime after 5pm right so that i need to')\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte_pair_tokenizer.all_merges\n",
    "# byte_pair_tokenizer.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "out_dir = os.path.join(os.path.curdir, \"output\")\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "tokens_dir = os.path.join(out_dir, \"tokens.txt\")\n",
    "merges_dir = os.path.join(out_dir, \"merge_rule.txt\")\n",
    "tokenized_samples_dir = os.path.join(out_dir, \"tokenized_samples.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tokens_dir, 'w') as f:\n",
    "    for token in byte_pair_tokenizer.vocabulary:\n",
    "        f.write(token + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(merges_dir, 'w') as f:\n",
    "    for merge_pair in byte_pair_tokenizer.all_merges:\n",
    "        f.write(merge_pair[0] + ',' + merge_pair[1] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'this is an nlp course',\n",
    "    'i love nlp',\n",
    "    'how are you doing today',\n",
    "    'my name is khushdev'\n",
    "]\n",
    "\n",
    "with open(tokenized_samples_dir, 'w') as f:\n",
    "    for sentence in test_sentences:\n",
    "        tokens = byte_pair_tokenizer.tokenize(sentence)\n",
    "        f.write(','.join(tokens) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
